# 行为识别之骨架动作行为识别

## 1.**基于骨架数据的人体行为识别的**简介



### 1.人体行为识别

> 动图

​         人体行为识别是指通过特定的算法，从人体本身的特征和所完成动作的特征出发，识别出人物具体动作的分类任务。它在智能监控、视频检索等方面有着广泛的应用。准确地提取到每个行为的语义信息，来描述其空间及时间上的动态变化，是这个方向的主要挑战。

### 2.基于骨架数据的人体行为识别



![image-20231124160552639](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20231124160552639.png)

​         随着 Kinect 深度相机的出现和人体姿态估计算法的发展，研究人员可以轻松地获取人体的3D骨架关键点信息，骨架序列的采集变得更加便捷、成本更低，使骨架数据具有计算量小的优点。与传统的RGB视频数据相比，人体骨骼有着更完整的结构信息，可以避免背景、光照以及视角变化所产生的噪声影响，对复杂的场景有着更强的鲁棒性，可以大大地提高识别效率。凭借这些优势，基于骨架数据的人体行为识别具有深远的研究意义。

![image-20231124160747597](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20231124160747597.png)

> 动图



### 3.骨架动作行为识别发展历程



![image-20231124162939949](../assets/image-20231124162939949.png)

​																	发展历程图

​         基于传统的骨架行为识别方法利用手工提取特征方法，通过一系列三维操作（如关节旋转和平移）手动提取特征，使用人类工程特征检测器和描述符来构建特征向量。然后通过训练通用分类器来执行分类。 

![image-20231124163843373](../assets/image-20231124163843373.png)



​         **基于深度学习方法主要分为基于卷积神经网络的行为识别方法、基于循环神经网络的行为识别方法和基于图卷积网络的行为识别方法；**



#### 3.1基于卷积神经网络（CNN）的行为识别方法



​        基于CNN的方法具备学习高级语义线索的能力，广泛应用于处理2D图像。基于卷积神经网络的行为识别方法一般将人体骨架序列从矢量序列转换为伪图像，伪图像的宽度、高度和通道分别代表骨架序列的关节、帧和坐标，形成伪图像，其中行代表不同的关节，列代表不同的帧，随后通过卷积神经网络进行动作识别。

![image-20231124164101048](../assets/image-20231124164101048.png)



#### 3.2基于循环神经网络（RNN）的行为识别方法

​         循环神经网络在处理可变长度的序列数据方面具备更强大的能力，更适合模拟动作的运动和时间依赖性。基于循环神经网络RNNs的方法通常将关节数据表示为若干向量序列，其中每个向量序列包括了每帧中各关节的位置数据，随后这些数据被馈送到循环神经网络中以进行动作识别。

![image-20231124164148386](../assets/image-20231124164148386.png)



>  总结:基于CNN和RNN方法都不能很好的处理非欧式空间中的图数据，因为将图结构数据转换为伪图像或坐标向量再进行卷积会损失原始的信息。
>
> ​	CNN更加适合处理规则的网格数据，比如图像（由像素点构成），但是它很难对图结构数据进行有效的卷积。
>
> ​	RNN适合处理时序性的问题，但是很难感知骨架数据的空间构型，也很难有效的建模长时间序列上关节点之间的依赖关系。



#### 3.3基于图卷积网络（GCN）的行为识别方法



  人体骨骼是非欧几里德的图数据，将关节根据骨骼联接构成图，每一帧的骨架是一个独立的图，作为空间信息。沿着时间维度连接骨架关节来扩展空间图，以构建时空图。经过多层卷积提取空间和时间特征，最后再分类。

![image-20231124164529713](../assets/image-20231124164529713.png)





## 2.基于骨架数据的人体行为识别的数据集



### 2.1数据集介绍

本方向的常用数据集主要包括：NTU-RGB+D60，NTU-RGB+D120，Kinetics400，以及NW-UCLA等

以最为常见的NTU-RGB+D60 数据集为例：

NTU-RGB+D60 数据集采集到的关节点为25个。数据集内包含若干“.*skeleton*”文件，每个文件代表一个样本。

![image-20231124164617216](../assets/image-20231124164617216.png)

**文件命名方式如下：**

•S后面跟的是**设置号**（1-17）

•C后面跟的是**相机****ID**（1-3）

•P后面跟的是**人物****ID**（1-40）

•R后面跟的是**动作执行的遍数**（1-2）

•A后面跟的是**动作的分类**（1-60）





![image-20231124164628923](../assets/image-20231124164628923.png)

**骨架轮廓图【每个都有25个骨骼点】**

  NTU-RGB+D60数据集包含60个种类的动作，共56880个样本，其中有40类为日常行为动作，11类为双人互动的动作。该数据集由微软 Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括深度信息、3D骨架信息、RGB视频以及红外序列。



  NTU-RGB+D60数据集包含60个种类的动作，共56880个样本，其中有40类为日常行为动作，11类为双人互动的动作。该数据集由微软 Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括深度信息、3D骨架信息、RGB视频以及红外序列。

![image-20231124164744736](../assets/image-20231124164744736.png)

​													**NTU-RGB+D的60个动作类别图**



- 102表示共有102帧3d骨架图；
- 1是指单张骨架图中有多少骨架
- 72057594037931691是指追踪的骨架编号；
- 接下来是6个整型数，分别是clipedEdges，handLeftConfidence，handLeftState，handRightConfidence，handRightState，isResticted，该行接下来是骨架的x，y偏置（骨架位置？），该行最后一个数字是该骨架的trackingState（？）；25表示有25个关键点（关节）信息，接下来有25行；
- 表示每帧每个关节点有12个数据，分别代表——**'x', 'y', 'z',** 'depthX', 'depthY', 'colorX', 'colorY','orientationW', 'orientationX', 'orientationY','orientationZ', 'trackingState'。
- 每个关键点信息行上有11个数据，前三个是关键点的3D位置（x，y，z），第四五个是在2D帧中的位置（用于匹配IR帧），第六第七个是在2D帧中的位置（用于匹配RGB帧），第八第九第十第十一是该关键点的方向（WXYZ）第十二个是该关键点的trackingState是keleton的60个动作类别

![image-20231124164854085](../assets/image-20231124164854085.png)

​    										   骨骼数据文件详细图

### 2.2数据集可视化

![image-20231124215750042](../assets/image-20231124215750042.png)



Action : **hugging**



> 动图



Action : **cheer up**



> 动图





## 3.基于骨架数据的人体行为识别的经典算法

类比图片的分类问题，我们将图片的数据集（常见的有Cifar10，COCO，Imagenet）封装成 (N,C,H,W)，分别代表batchsize，通道数（RGB的数值），图片的宽和高。

骨架数据是不规则的图结构。我们通过循环遍历.skeleton文件，读取其中的 (x,y,z) 的坐标信息，并将其封装成可以输入进网络的格式 (N,C,T,V,M)，其中：

N 代表batchsize，

C 代表通道数，通常为关节点的坐标（x,y,z）

T 代表帧数，

V 代表关节点数，

M 代表执行本动作的人数（单人动作或双人动作）。

因为动作是在时间上是持续的，所以骨骼数据多出了一个时间维度T。

> 那么如何处理关节点在空间维度上以及时间维度上的动作变化呢？

### 3.1STGCN



![image-20231124215935538](../assets/image-20231124215935538.png)

![image-20231124215940166](../assets/image-20231124215940166.png)







该篇论文于2018年发表在AAAI上。简称ST-GCN。

论文的主要思想：在骨架序列上构造时空图，应用多层时空图卷积(spatial-temporal graph convolution)，逐步在图上生成更高层次的特征图 （feature map）。最后使用Softmax分类器分类到相应的动作类别。

这是GCN首次应用于基于骨架数据的人体行为识别，后续工作中端到端的模型大多都建立在它的基础上。



> 

![image-20231124220004203](../assets/image-20231124220004203.png)



这篇文章引用量已经达到3450次，足以可见其影响力。

![image-20231124215951844](../assets/image-20231124215951844.png)

> 引用数

### 3.2具体方法

作者开创性地设计了一种骨架序列通用表示，即同一帧之内不同节点的连接和不同帧之间同一节点的连接，命名为骨架时空图，在骨架时空图的基础上将图卷积操作融入行为识别，构建出了时空图卷积网络模型，允许特征沿着空间和时间维度进行整合。



蓝色点表示关节点；



各关节点之间的边（即图中颜色为蓝色的边）是由人体骨架的自然连接定义的，即**空间边**。



帧之间的边（即图中颜色为绿色的边）连接前后时间帧之间的相同关节，即**时间边**。



![image-20231124220206335](../assets/image-20231124220206335.png)











引入一个可学习的权重矩阵（与邻接矩阵等大小）与邻接矩阵按位相乘。用来赋予邻接矩阵中重要边（节点）较大的权重且抑制非重要边（节点）的权重。

将加权后的邻接矩阵与骨架序列送至GCN中进行运算。



利用TCN网络，实现时间维度信息的聚合。

引入了残差结构（一个CNN+BN）计算获得Res，与GCN的输出按位相加，实现空间维度信息的聚合。

![image-20231124220320922](../assets/image-20231124220320922.png)





## 4基于骨架数据的人体行为识别的学习路线



### 4.1基于骨架的行为识别的发展方向![image-20231124220341280](../assets/image-20231124220341280.png)



### 4.2学习路线



从基本的ST-GCN的代码入手（https://github.com/yysijie/st-gcn）

1. 深入理解数据集（从.skeleton文件中提取需要的关节点信息）

（https://blog.csdn.net/weixin_51450749/article/details/111768242）

2. 学会可视化数据集

3. 利用**Dataset**和**Dataloader**模块进行封装 ——>**（****N,C,T,V,W****）**

4. 分析网络结构（给定网络一个输入，得到输出，利用一些可视化的第三方库，帮助理解）

5. 学习**模型训练方式**（定义模型，损失，优化器等，保存模型训练过程中的信息，比如准确率，权重等）

6. 为了能在服务器上训练模型，要学会简单的部署和运维，熟悉一些基础的Linux命令行。